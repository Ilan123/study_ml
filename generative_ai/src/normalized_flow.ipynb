{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c977bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Subset\n",
    "from torch import Tensor\n",
    "from typing import Tuple, Callable\n",
    "from itertools import chain\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0ded18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root='../data',\n",
    "    train=True,\n",
    "    # download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='../data',\n",
    "    train=False,\n",
    "    # download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1af4652",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = 0\n",
    "training_data_incides = torch.where(training_data.targets == target_label)[0]\n",
    "test_data_incides = torch.where(test_data.targets == target_label)[0]\n",
    "\n",
    "training_data = Subset(training_data, training_data_incides)\n",
    "test_data = Subset(test_data, test_data_incides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143c8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderWrapper:\n",
    "    def __init__(self, dl: DataLoader, desiried_labels: torch.Tensor):\n",
    "        self.dl = dl\n",
    "        self.desiried_labels = torch.tensor(desiried_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for X, y in self.dl:\n",
    "            mask = torch.isin(y, self.desiried_labels)\n",
    "            yield X[mask], y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b973d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([32, 1, 28, 28])\n",
      "Shape of y: torch.Size([32]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoaderWrapper(DataLoader(training_data, batch_size=batch_size), [0])\n",
    "test_dataloader = DataLoaderWrapper(DataLoader(test_data, batch_size=batch_size), [0])\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a196257",
   "metadata": {},
   "source": [
    "$$\n",
    "z_B = \\exp\\left(-s(z_A)\\right) \\odot \\left(x_B - b(z_A)\\right)  \n",
    "$$\n",
    "\n",
    "$$\n",
    "J = \n",
    "\\begin{bmatrix}\n",
    "I_d & 0 \\\\\n",
    "\\frac{\\partial z_B}{\\partial x_A} & \\mathrm{diag}\\big(\\exp(-s)\\big)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_B = \\exp\\big(s(z_A, w)\\big) \\odot z_B + b(z_A, w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cca5cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        split_at: int,\n",
    "        scale_net: nn.Module, # s\n",
    "        shift_net: nn.Module, # b\n",
    "        alternate_parts: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.split_at = split_at\n",
    "        self.scale_net = scale_net\n",
    "        self.shift_net = shift_net\n",
    "        self.alternate_parts = alternate_parts\n",
    "    \n",
    "\n",
    "    def _split(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        if self.alternate_parts:\n",
    "            return x[:, self.split_at:], x[:, :self.split_at]\n",
    "        else:\n",
    "            return x[:, :self.split_at], x[:, self.split_at:]\n",
    "\n",
    "\n",
    "    def _merge(self, xA: Tensor, xB: Tensor) -> Tensor:\n",
    "        if self.alternate_parts:\n",
    "            return torch.cat((xB, xA), dim=1)\n",
    "        else:\n",
    "            return torch.cat((xA, xB), dim=1)\n",
    "\n",
    "\n",
    "    def _get_scale_and_shift(self, zA: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        log_scale = self.scale_net(zA)\n",
    "        log_scale = torch.clamp(log_scale, min=self.log_scale_min, max=self.log_scale_max)\n",
    "        shift = self.shift_net(zA)\n",
    "        return log_scale, shift\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, log_det_total: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        xA, xB = self._split(x)\n",
    "        zA = xA\n",
    "        log_scale, shift = self._get_scale_and_shift(zA)\n",
    "\n",
    "        zB = torch.exp(-log_scale) * (xB - shift)\n",
    "        z = self._merge(zA, zB)\n",
    "\n",
    "        log_det_current = -torch.sum(log_scale, dim=1)\n",
    "        log_det_total = log_det_total + log_det_current\n",
    "        return z, log_det_total\n",
    "\n",
    "\n",
    "    def inverse(self, z: Tensor) -> Tensor:\n",
    "        zA, zB = self._split(z)\n",
    "        xA = zA\n",
    "        log_scale, shift = self._get_scale_and_shift(zA)\n",
    "\n",
    "        xB = torch.exp(log_scale) * (zB + shift)\n",
    "        x = self._merge(xA, xB)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be0e4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivationStack(nn.Module):\n",
    "    def __init__(self,\n",
    "        num_of_layers: int,\n",
    "        in_features: int,\n",
    "        activation_layer: nn.Module = nn.ReLU(),\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.activation_layer = activation_layer\n",
    "        layers = chain.from_iterable(\n",
    "            (\n",
    "                nn.Linear(in_features, in_features, bias=bias, device=device, dtype=dtype),\n",
    "                # Clone to avoid shared state\n",
    "                copy.deepcopy(activation_layer)\n",
    "            )\n",
    "            for _ in range(num_of_layers)\n",
    "        )\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        return self.model(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49979d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_layer = nn.ReLU()\n",
    "layers = chain.from_iterable(\n",
    "            (\n",
    "                nn.Linear(4, 4, bias=True),\n",
    "                # Clone to avoid shared state\n",
    "                copy.deepcopy(nn.ReLU())\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        )\n",
    "\n",
    "nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c2f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dActivationStack(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_of_layers: int,\n",
    "        in_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        padding: int = 1,\n",
    "        activation_layer: nn.Module = nn.ReLU(),\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        input_is_flat: bool = False,      # New flag\n",
    "        image_shape: tuple[int, int, int] = None  # (C, H, W)\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.activation_layer = activation_layer\n",
    "        self.input_is_flat = input_is_flat\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "        layers = chain.from_iterable(\n",
    "            (\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    in_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    bias=bias,\n",
    "                    device=device,\n",
    "                    dtype=dtype,\n",
    "                ),\n",
    "                # Clone to avoid shared state\n",
    "                copy.deepcopy(activation_layer) \n",
    "            )\n",
    "            for _ in range(num_of_layers)\n",
    "        )\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        if self.input_is_flat:\n",
    "            assert (\n",
    "                self.image_shape is not None\n",
    "            ), \"Must provide image_shape if input_is_flat is True\"\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.input_is_flat:\n",
    "            # reshape flat vector to image tensor\n",
    "            B = x.size(0)\n",
    "            x = x.view(B, *self.image_shape)  # (B, C, H, W)\n",
    "\n",
    "        x = self.model(x)\n",
    "\n",
    "        if self.input_is_flat:\n",
    "            # flatten back to vector\n",
    "            B = x.size(0)\n",
    "            x = x.view(B, -1)  # (B, C*H*W)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48338750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        func: Callable[[Tensor], Tensor],\n",
    "        inv_func: Callable[[Tensor], Tensor],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        self.inv_func = inv_func\n",
    "\n",
    "    def forward(self, x: Tensor, *args, **kwargs) -> Tensor:\n",
    "        y = self.func(x)\n",
    "        if args or kwargs:\n",
    "            return (y, *args) if not kwargs else (y, *args, kwargs)\n",
    "        return y\n",
    "    \n",
    "    def inverse(self, x: Tensor) -> Tensor:\n",
    "        return self.inv_func(x)\n",
    "\n",
    "\n",
    "def vec_to_img(x: Tensor) -> Tensor:\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "def img_to_vec(x: Tensor) -> Tensor:\n",
    "    return x.flatten(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b40c6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowSequential(nn.Module):\n",
    "    def __init__(self, *modules: nn.Module):\n",
    "        super().__init__()\n",
    "        self.modules_list = nn.ModuleList(modules)\n",
    "\n",
    "        # Check if all modules implement required interface\n",
    "        for i, module in enumerate(self.modules_list):\n",
    "            if not hasattr(module, 'forward') or not hasattr(module, 'inverse'):\n",
    "                raise TypeError(f\"Module at index {i} must implement both 'forward' and 'inverse' methods.\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        log_det:\n",
    "        torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        for module in self.modules_list:\n",
    "            x, log_det = module(x, log_det)\n",
    "        return x, log_det\n",
    "\n",
    "    def inverse(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        for module in reversed(self.modules_list):\n",
    "            z = module.inverse(z)\n",
    "        return z\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        log_det: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Optional: put extra logic here before/after forward\n",
    "        return self.forward(x, log_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7633f536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d42af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlowSequential(\n",
    "    LambdaLayer(img_to_vec, vec_to_img),\n",
    "    CouplingLayer(392, LinearActivationStack(2, 392, device = device), LinearActivationStack(2, 392, device = device)),\n",
    "    CouplingLayer(392, LinearActivationStack(2, 392, device = device), LinearActivationStack(2, 392, device = device), True)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b59912d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LambdaLayer.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mFlowSequential.__call__\u001b[39m\u001b[34m(self, x, log_det)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     28\u001b[39m     x: torch.Tensor,\n\u001b[32m     29\u001b[39m     log_det: torch.Tensor\n\u001b[32m     30\u001b[39m ) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Optional: put extra logic here before/after forward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_det\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mFlowSequential.forward\u001b[39m\u001b[34m(self, x, log_det)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     13\u001b[39m     x: torch.Tensor,\n\u001b[32m     14\u001b[39m     log_det:\n\u001b[32m     15\u001b[39m     torch.Tensor\n\u001b[32m     16\u001b[39m ) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.modules_list:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         x, log_det = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_det\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x, log_det\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: LambdaLayer.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "model(training_data[0][0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f2416e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LambdaLayer(img_to_vec, vec_to_img)(training_data[0][0]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
