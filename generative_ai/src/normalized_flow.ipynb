{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c977bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Subset\n",
    "from torch import Tensor\n",
    "from typing import Tuple, Callable\n",
    "from itertools import chain\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f7978e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29824/3266725043.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "scaler = GradScaler(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0ded18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root='../data',\n",
    "    train=True,\n",
    "    # download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='../data',\n",
    "    train=False,\n",
    "    # download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1af4652",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = 0\n",
    "training_data_incides = torch.where(training_data.targets == target_label)[0]\n",
    "test_data_incides = torch.where(test_data.targets == target_label)[0]\n",
    "\n",
    "training_data = Subset(training_data, training_data_incides)\n",
    "test_data = Subset(test_data, test_data_incides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b2abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define validation split fraction\n",
    "val_fraction = 0.1\n",
    "dataset_size = len(training_data)\n",
    "val_size = int(val_fraction * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "# Create a reproducible shuffled list of indices\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "indices = torch.randperm(dataset_size, generator=generator).tolist()\n",
    "\n",
    "# Split indices for train and validation\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "# Wrap Subsets for train and validation datasets\n",
    "train_data = Subset(training_data, train_indices)\n",
    "val_data = Subset(training_data, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b973d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "Training batches: 94\n",
      "Test batches: 16\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "print(f\"Training batches: {len(train_dataloader)}\")\n",
    "print(f\"Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a196257",
   "metadata": {},
   "source": [
    "$$\n",
    "z_B = \\exp\\left(-s(z_A)\\right) \\odot \\left(x_B - b(z_A)\\right)  \n",
    "$$\n",
    "\n",
    "$$\n",
    "J = \n",
    "\\begin{bmatrix}\n",
    "I_d & 0 \\\\\n",
    "\\frac{\\partial z_B}{\\partial x_A} & \\mathrm{diag}\\big(\\exp(-s)\\big)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_B = \\exp\\big(s(z_A, w)\\big) \\odot z_B + b(z_A, w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca5cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        split_at: int,\n",
    "        scale_net: nn.Module, # s\n",
    "        shift_net: nn.Module, # b\n",
    "        alternate_parts: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.split_at = split_at\n",
    "        self.scale_net = scale_net\n",
    "        self.shift_net = shift_net\n",
    "        self.alternate_parts = alternate_parts\n",
    "    \n",
    "\n",
    "    def _split(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        if self.alternate_parts:\n",
    "            return x[:, self.split_at:], x[:, :self.split_at]\n",
    "        else:\n",
    "            return x[:, :self.split_at], x[:, self.split_at:]\n",
    "\n",
    "\n",
    "    def _merge(self, xA: Tensor, xB: Tensor) -> Tensor:\n",
    "        if self.alternate_parts:\n",
    "            return torch.cat((xB, xA), dim=1)\n",
    "        else:\n",
    "            return torch.cat((xA, xB), dim=1)\n",
    "\n",
    "\n",
    "    def _get_scale_and_shift(self, zA: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        log_scale = self.scale_net(zA)\n",
    "        log_scale = torch.clamp(log_scale, min=-5, max=3)\n",
    "        shift = self.shift_net(zA)\n",
    "        return log_scale, shift\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, log_det_total: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        xA, xB = self._split(x)\n",
    "        zA = xA\n",
    "        log_scale, shift = self._get_scale_and_shift(zA)\n",
    "\n",
    "        zB = torch.exp(-log_scale) * (xB - shift)\n",
    "        z = self._merge(zA, zB)\n",
    "\n",
    "        log_det_current = -torch.sum(log_scale, dim=1)\n",
    "        log_det_total = log_det_total + log_det_current\n",
    "        return z, log_det_total\n",
    "\n",
    "\n",
    "    def inverse(self, z: Tensor) -> Tensor:\n",
    "        zA, zB = self._split(z)\n",
    "        xA = zA\n",
    "        log_scale, shift = self._get_scale_and_shift(zA)\n",
    "\n",
    "        xB = torch.exp(log_scale) * (zB + shift)\n",
    "        x = self._merge(xA, xB)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "        layers_dims: list,\n",
    "        activation_layer: nn.Module = nn.ReLU(),\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert len(layers_dims) > 2\n",
    "        layers = []\n",
    "        \n",
    "        for in_features, out_features in zip(layers_dims[:-2], layers_dims[1:-1]):\n",
    "            layers.append(nn.Linear(in_features, out_features, bias=bias, device=device, dtype=dtype))\n",
    "            layers.append(copy.deepcopy(activation_layer))\n",
    "        \n",
    "        layers.append(nn.Linear(layers_dims[-2], layers_dims[-1], bias=bias, device=device, dtype=dtype))\n",
    "\n",
    "        # Init last layer to small values for stability\n",
    "        nn.init.zeros_(layers[-1].weight)\n",
    "        if bias:\n",
    "            nn.init.zeros_(layers[-1].bias)\n",
    "            \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        return self.model(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        func: Callable[[Tensor], Tensor],\n",
    "        inv_func: Callable[[Tensor], Tensor],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        self.inv_func = inv_func\n",
    "\n",
    "    def forward(self, x: Tensor, *args, **kwargs):\n",
    "        y = self.func(x)\n",
    "        if args or kwargs:\n",
    "            return (y, *args) if not kwargs else (y, *args, kwargs)\n",
    "        return y\n",
    "    \n",
    "    def inverse(self, x: Tensor) -> Tensor:\n",
    "        return self.inv_func(x)\n",
    "\n",
    "\n",
    "def vec_to_img(x: Tensor) -> Tensor:\n",
    "    return x.view(-1, 28, 28)\n",
    "\n",
    "def img_to_vec(x: Tensor) -> Tensor:\n",
    "    return x.flatten(1)\n",
    "\n",
    "\n",
    "def sigmoid_inverse(x: Tensor):\n",
    "    # Avoid log(0)\n",
    "    x = torch.clamp(x, 1e-6, 1 - 1e-6)\n",
    "    x = torch.log(x) - torch.log(1 - x)\n",
    "    return x\n",
    "\n",
    "def add_noise(x: Tensor):\n",
    "    # Add uniform noise for dequantization (important for discrete data)\n",
    "    return x + torch.rand_like(x) / 256.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowSequential(nn.Module):\n",
    "    def __init__(self, data_dim: int, layers: nn.Module):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        # Check if all modules implement required interface\n",
    "        for i, module in enumerate(self.layers):\n",
    "            if not hasattr(module, 'forward') or not hasattr(module, 'inverse'):\n",
    "                raise TypeError(f\"Module at index {i} must implement both 'forward' and 'inverse' methods.\")\n",
    "            \n",
    "        self.register_buffer('base_mean', torch.zeros(data_dim))\n",
    "        self.register_buffer('base_cov', torch.eye(data_dim))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        log_det: torch.Tensor = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if log_det is None:\n",
    "            log_det = torch.zeros(len(x), device=x.device)\n",
    "        for module in self.layers:\n",
    "            x, log_det = module(x, log_det)\n",
    "        return x, log_det\n",
    "\n",
    "    def inverse(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        for module in reversed(self.layers):\n",
    "            z = module.inverse(z)\n",
    "        return z\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        log_det: torch.Tensor = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Optional: put extra logic here before/after forward\n",
    "        return self.forward(x, log_det)\n",
    "    \n",
    "    def sample(self, num_samples: int, device:str=device) -> Tensor:\n",
    "        base_dist = MultivariateNormal(self.base_mean, self.base_cov)\n",
    "        z = base_dist.sample((num_samples,)).to(device)\n",
    "        \n",
    "        # Transform through inverse flow\n",
    "        with torch.no_grad():\n",
    "            x = self.inverse(z)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def log_prob(self, x: Tensor):\n",
    "        z, log_det = self.forward(x)\n",
    "        \n",
    "        base_dist = MultivariateNormal(self.base_mean, self.base_cov)\n",
    "        log_prob_base = base_dist.log_prob(z)\n",
    "        \n",
    "        log_prob = log_prob_base + log_det\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fabbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For implementation challange\n",
    "NORMAL_DIST_CONST = torch.log(torch.tensor(2 * torch.pi, device=device))\n",
    "def normal_NLL(X: Tensor) -> Tensor:\n",
    "    \"\"\"Compute -log p(z) for standard Gaussian\"\"\"\n",
    "    d = X.shape[1]\n",
    "    const_term = (d / 2) * NORMAL_DIST_CONST.to(X.device)\n",
    "    squared_term = 0.5 * X.pow(2).sum(dim=1)\n",
    "    return const_term + squared_term\n",
    "\n",
    "\n",
    "def flow_NLL_loss(\n",
    "    z_sample: Tensor,\n",
    "    total_log_det: Tensor,\n",
    ") -> Tensor:\n",
    "    \"\"\"Flow loss = base NLL - total log det (averaged)\"\"\"\n",
    "    return torch.mean(normal_NLL(z_sample) - total_log_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "        model: nn.Module,\n",
    "        data_loader: DataLoader,\n",
    "        device: str,\n",
    "        optimizer,\n",
    "        training_mode: bool = True,\n",
    "    ) -> float:\n",
    "    \"\"\"\n",
    "    Run a single epoch in either training or evaluation mode.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train/evaluate\n",
    "        dataloader: DataLoader for the dataset\n",
    "        device: Device to run on (cuda/cpu)\n",
    "        optimizer: Optimizer for training (required if train_mode=True)\n",
    "        train_mode: If True, run in training mode; if False, run in evaluation mode\n",
    "        max_grad_norm: Maximum gradient norm for clipping (only used in training)\n",
    "    \n",
    "    Returns:\n",
    "        average_loss: Average loss for the epoch\n",
    "    \"\"\"\n",
    "    total_loss, num_batches = 0, 0\n",
    "    \n",
    "    for data, _ in tqdm(data_loader, total=len(data_loader)):\n",
    "        data = data.to(device)\n",
    "        if training_mode:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Compute negative log likelihood\n",
    "        log_prob = model.log_prob(data)\n",
    "        loss = -log_prob.mean()\n",
    "        \n",
    "        if training_mode:\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_with_train_state(\n",
    "    checkpoint_path: str,\n",
    "    model: nn.Module,\n",
    "    epoch: int,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train_avg_loss: float,\n",
    "    val_avg_loss: float,\n",
    "    train_losses: list,\n",
    "    val_losses: list,\n",
    "):\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_avg_loss,\n",
    "        'val_loss': val_avg_loss,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }, checkpoint_path)\n",
    "\n",
    "\n",
    "def load_best_model(\n",
    "        model: nn.Module,\n",
    "        checkpoint_path: str,\n",
    "        device: str\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Load the best saved model from checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        model: The model architecture (should match the saved model)\n",
    "        checkpoint_path: Path to the saved checkpoint\n",
    "        device: Device to load the model on\n",
    "        \n",
    "    Returns:\n",
    "        The loaded model\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"Loaded model from epoch {checkpoint['epoch']} with validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(model, device, epoch=None):\n",
    "    \"\"\"Generate and visualize samples from the trained model\"\"\"\n",
    "    # Sample\n",
    "    model.eval()\n",
    "    samples = model.sample(16, device).cpu()\n",
    "    samples = torch.clamp(samples, 0, 1)\n",
    "    \n",
    "    # Plot samples\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(samples[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    title = f'Generated Samples - Epoch {epoch}' if epoch else 'Generated Samples'\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_flow(\n",
    "        model: nn.Module,\n",
    "        train_dataloader: DataLoader,\n",
    "        val_dataloader: DataLoader,\n",
    "        device: str,\n",
    "        num_epochs: int = 50,\n",
    "        lr: float = 1e-4,\n",
    "        patience: int = 10,\n",
    "        save_path: str = \"../models/best_model.pth\"\n",
    "    ) -> Tuple[list, list]:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_avg_loss = run_epoch(\n",
    "            model, train_dataloader, device, optimizer, True)\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_avg_loss = run_epoch(\n",
    "                model, val_dataloader, device, optimizer, False)\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_losses.append(train_avg_loss)\n",
    "        val_losses.append(val_avg_loss)\n",
    "\n",
    "\n",
    "        if val_avg_loss < best_val_loss:\n",
    "            best_val_loss = val_avg_loss\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_without_improvement = 0\n",
    "\n",
    "            save_model_with_train_state(\n",
    "                save_path, model, epoch, optimizer, scheduler,\n",
    "                train_avg_loss, val_avg_loss, train_losses, val_losses,\n",
    "            )\n",
    "            \n",
    "            print(f'Epoch {epoch+1} completed. Train Loss: {train_avg_loss:.4f}, '\n",
    "                  f'Val Loss: {val_avg_loss:.4f} # NEW BEST MODEL #')\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f'Epoch {epoch+1} completed. Train Loss: {train_avg_loss:.4f}, '\n",
    "                  f'Val Loss: {val_avg_loss:.4f} (Best: {best_val_loss:.4f} at epoch {best_epoch})')\n",
    "        \n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            visualize_samples(model, device, epoch+1)\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'\\nEarly stopping triggered after {patience} epochs without improvement.')\n",
    "            print(f'Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}')\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194cd943",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_net = FlowSequential(\n",
    "    784,\n",
    "    [\n",
    "        LambdaLayer(img_to_vec, vec_to_img),\n",
    "        LambdaLayer(add_noise, lambda x: x),\n",
    "        LambdaLayer(sigmoid_inverse, torch.sigmoid),\n",
    "        CouplingLayer(392, MLP([392, 512, 512, 392], device = device), MLP([392, 512, 512, 392], device = device)),\n",
    "        CouplingLayer(392, MLP([392, 512, 512, 392], device = device), MLP([392, 512, 512, 392], device = device), True),\n",
    "        CouplingLayer(392, MLP([392, 512, 512, 392], device = device), MLP([392, 512, 512, 392], device = device)),\n",
    "        CouplingLayer(392, MLP([392, 512, 512, 392], device = device), MLP([392, 512, 512, 392], device = device), True),\n",
    "        CouplingLayer(392, MLP([392, 512, 512, 392], device = device), MLP([392, 512, 512, 392], device = device)),\n",
    "        CouplingLayer(392, MLP([392, 512, 512, 392], device = device), MLP([392, 512, 512, 392], device = device), True),\n",
    "    ]\n",
    "\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844699de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "x_sample = next(iter(train_dataloader))[0].to(device)\n",
    "x_to_z = flow_net(x_sample, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c462050",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '../models/coupling_model.pth'\n",
    "train_losses, val_losses = train_flow(\n",
    "    flow_net, train_dataloader, val_dataloader, device,\n",
    "    num_epochs=10, lr=1e-4, save_path=model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
